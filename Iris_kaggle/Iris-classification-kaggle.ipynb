{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris-classification \n",
    "\n",
    "This side project is my **first** machine learning side project in which I can practice the framework of Gluon. I choose this problem because this is a very basic classification problem, which is a good starting point for myself. \n",
    "\n",
    "details about this project: https://www.kaggle.com/uciml/iris;  Info about MXNET and Gluon: http://gluon.mxnet.io/\n",
    "\n",
    "<cite>Data description from Kaggle</cite>\n",
    ">The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n",
    "\n",
    ">It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\"\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Preliminaries: Loading required libraries and importing data\n",
    "2. Preparing data: Scaling, cleaning and converting data\n",
    "3. Define Classifier: simple linear regression model\n",
    "4. Validate performance of classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Preliminaries\n",
    "\n",
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import ndarray as nd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data \n",
    "\n",
    "The columns in this dataset are:\n",
    "\n",
    "- **Id**: unique index of each sample\n",
    "- **SepalLengthCm**: feature of sepal length\n",
    "- **SepalWidthCm**: feature of sepal width\n",
    "- **PetalLengthCm**: feature of petal length\n",
    "- **PetalWidthCm**: feature of petal width\n",
    "- **Species**: label of Iris species     (integer indicates: 0 - Iris-setosa; 1 - Iris-versicolor; 2 - Iris-virginic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0   1            5.1           3.5            1.4           0.2        0\n",
       "1   2            4.9           3.0            1.4           0.2        0\n",
       "2   3            4.7           3.2            1.3           0.2        0\n",
       "3   4            4.6           3.1            1.5           0.2        0\n",
       "4   5            5.0           3.6            1.4           0.2        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Prepare the data\n",
    "\n",
    "### 1. merge *feature columns* of both training and testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = pd.concat((train.loc[:, 'SepalLengthCm':'PetalWidthCm'],\n",
    "        test.loc[:,'SepalLengthCm':'PetalWidthCm']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. scale the features to avoid skewness as:\n",
    "\n",
    "$$x =  \\frac{x - E_x}{ \\sigma {x}} $$\n",
    "where $E_x$ is the expectation of $x$, and $\\sigma{x}$ is the standard deviation of $x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feas = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "all_X[numeric_feas] = all_X[numeric_feas].apply(lambda x: (x - x.mean()) / (x.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. convert categorical variable into indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = pd.get_dummies(all_X, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. convert format into matrix as input of ndarray and mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 2.]\n",
       " [ 2.]\n",
       " [ 0.]\n",
       " [ 0.]\n",
       " [ 0.]\n",
       " [ 0.]\n",
       " [ 1.]\n",
       " [ 1.]]\n",
       "<NDArray 8x1 @cpu(0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of samples in training set\n",
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "y_train = train.Species.as_matrix()\n",
    "# load training data into ndarray\n",
    "X_train = nd.array(X_train)\n",
    "y_train = nd.array(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "\n",
    "# number of samples in testing set\n",
    "num_test = test.shape[0]\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_test = test.Species.as_matrix()\n",
    "# load testing data into ndarray\n",
    "X_test = nd.array(X_test)\n",
    "y_test = nd.array(y_test)\n",
    "y_test.reshape((num_test, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Define Classifier\n",
    "\n",
    "* loss function\n",
    "* model\n",
    "* data iterator\n",
    "* trainer instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(1))\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. define data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gluon.data.ArrayDataset(X_train, y_train)\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. define trainer instance\n",
    "\n",
    "Note the learning rate here is very important! It affects the convergence rate and accuracy of classification. Try to tune it and see the result change! üòÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "net.collect_params().initialize(force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Train the model and validate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Epoch 0, average training loss: 0.000018\n",
      "Epoch 0, average testing loss: 0.035717\n",
      "-----------------------------------------\n",
      "Epoch 1, average training loss: 0.000018\n",
      "Epoch 1, average testing loss: 0.035707\n",
      "-----------------------------------------\n",
      "Epoch 2, average training loss: 0.000017\n",
      "Epoch 2, average testing loss: 0.035714\n",
      "-----------------------------------------\n",
      "Epoch 3, average training loss: 0.000016\n",
      "Epoch 3, average testing loss: 0.035708\n",
      "-----------------------------------------\n",
      "Epoch 4, average training loss: 0.000015\n",
      "Epoch 4, average testing loss: 0.035708\n",
      "-----------------------------------------\n",
      "Epoch 5, average training loss: 0.000014\n",
      "Epoch 5, average testing loss: 0.035729\n",
      "-----------------------------------------\n",
      "Epoch 6, average training loss: 0.000014\n",
      "Epoch 6, average testing loss: 0.035718\n",
      "-----------------------------------------\n",
      "Epoch 7, average training loss: 0.000013\n",
      "Epoch 7, average testing loss: 0.035729\n",
      "-----------------------------------------\n",
      "Epoch 8, average training loss: 0.000013\n",
      "Epoch 8, average testing loss: 0.035682\n",
      "-----------------------------------------\n",
      "Epoch 9, average training loss: 0.000012\n",
      "Epoch 9, average testing loss: 0.035692\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_test_loss = 0\n",
    "    total_sample = 0\n",
    "    for data, label in data_iter:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, y_train)\n",
    "        loss.backward()\n",
    "        total_sample += batch_size\n",
    "        trainer.step(batch_size)\n",
    "        # training error\n",
    "        total_loss += nd.sum(loss).asscalar()\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Epoch %d, average training loss: %f\" % (e, total_loss/total_sample))\n",
    "    # testing error\n",
    "    if X_test is not None:\n",
    "        test_output = net(X_test)\n",
    "        test_loss = square_loss(test_output, y_test)\n",
    "        total_test_loss += nd.sum(test_loss).asscalar()\n",
    "    print(\"Epoch %d, average testing loss: %f\" % (e, total_test_loss/total_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
